{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5d3d031-f6bb-45d4-9dbd-cb9ab7431298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Stream Customers Data From Cloud files to delta lake\n",
    "1. Read files from cloud storage using DataStreamReader API\n",
    "2. Transform Dataframe to the following:\n",
    "    i. file path: Cloud File path\n",
    "    ii. ingestion date: Current timestamp\n",
    "3. Write the transformed data stream to delta lake table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd9786f-81ae-4aa8-a508-8b1f97bbc623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. Read files using DataStreamReader API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89eed42-1016-45dd-8d17-5a888a994a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, MapType, TimestampType,DateType\n",
    "\n",
    "customer_schema = StructType(fields = [StructField(\"customer_id\", IntegerType()),\n",
    "                                       StructField(\"customer_name\", StringType()),\n",
    "                                       StructField(\"date_of_birth\",DateType()),\n",
    "                                       StructField(\"telephone\",StringType()),\n",
    "                                       StructField(\"email\", StringType()),\n",
    "                                       StructField(\"member_since\", DateType()),\n",
    "                                       StructField(\"created_timestamp\", DateType())\n",
    "                                       ]\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aca6d80-a090-4fe9-8edd-3f8d838071d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df = (\n",
    "                spark.readStream\n",
    "                     .format(\"json\")\n",
    "                     .schema(customer_schema)\n",
    "                     .load(\"/Volumes/shoppix/landing/operational_data/customer_stream/\")\n",
    ")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3120dc29-07af-403a-b656-b2c5c564fb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Transform the datafram to add following columns\n",
    "- file path: cloud file path\n",
    "- ingestion path: current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6a8f64-08da-4d0c-a1f5-e0a4f8bbb22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "customers_transformed_df = (\n",
    "                                customers_df.withColumn(\"file_path\", col(\"_metadata.file_path\"))\n",
    "                                            .withColumn(\"ingestiondate\",current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c5b08e8-089c-4697-a475-8a87d6d724bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. write transformed data to delta lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cc5f3e-2342-456c-961d-0901efee5aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "\n",
    "streaming_Query = (\n",
    "                    customers_transformed_df.writeStream\n",
    "                                            .format(\"delta\")\n",
    "                                            .option(\"checkpointLocation\",\"/Volumes/shoppix/landing/operational_data/customer_stream/_checkpoint_stream\")\n",
    "                                            .trigger(availableNow=True)\n",
    "                                            .toTable(\"shoppix.bronze.customer_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cb3888-de24-48fb-943b-9c4f147bea75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM shoppix.bronze.customer_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "191f4113-2c20-4aa9-b327-41e67d67e3e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7699288436354192,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "22. Ingest Customer stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
